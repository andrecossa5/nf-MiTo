# TUNE - EXPLORE - INFER tutorial

nf-MiTo INFER entrypoint (see [INFER tutorial](2.INFER.md)) performs all the operations (e.g., AFM pre-processing, character matrix bootstrapping and tree and clonal inference) required for lineage inference.

However, a single-combination of parameters only returns a *single* phylogeny. To make sure that our biological conclusion are robust to this parameter choice, it might be useful to perturb these parameters and explore different character spaces (with their associated phylogenies). Ideally, this *tuning* phase should be fast enough to lead to the "optimal" parameter combination. Visualization of cell phylogenies could also help this choice.

To facilitate all of these operations, nf-MiTo implements 3 composable workflows: TUNE, EXPLORE, and INFER. 

Here, we will demonstrate how to explore alternative MT-SNVs spaces using data from the `MDA_clones` sample (see [INFER tutorial](2.INFER.md)).

The whole analysis should take ~45 minutes (on an HPC environment).

# Prerequisites

These downstream workflows are not particularly memory intensive (i.e., a modern laptop with 16GB RAM and 8 cpus can handle all processes). However, being on a HPC computing cluster can significantly speed up all parallel operations (e.g., character matrices bootstrapping, distance calculations and tree inference), with multiple processes executed simoultaneously. 
As mentioned in the other tutorials, our only requirements include one between docker/apptainer/singularity in our $PATH, together with Nextflow.
We can check their availability with:

```bash
apptainer 
nextflow
```

# Download data

We start by downloding test data from zenodo. This includes the AFM and associated cell metadata
from a run of nf-MiTo PREPROCESS entrypoint (default options):

```bash
wget https://zenodo.org/records/17225334/files/data_test.tar.gz
```

# Prep data

Untar test_data:

```bash
tar -xzf data_test.tar.gz
```

Our `data_test` folder should look like:

```
data_test
├── afm_unfiltered.h5ad
└── cells_meta.csv
└── coverage.txt.gz
```

We have downloaded our test dataset. We are now ready to prep our `afm_input` file.

# Prep input file

We prep our <span style="white-space: nowrap;">`--afm_input`</span> CSV file following the usual template (see [INFER tutorial](2.INFER.md)):

<table>
<thead>
<tr>
<th align="left">job_id</th>
<th align="left">sample</th>
<th align="left">afm</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><code>test_0.1</code></td>
<td align="left"><code>MDA_clones<code></td>
<td align="left"><code>path to AFM</code></td>
</tr>
</tbody>
</table>

# TUNE 

We will start with the TUNE entrypoint. This workflow parallelize all lineage inference operations (i.e., cell and MT-SNVs filtering, genotyping, distance calculations, phylogeny and clonal inference) across the chosen parameter combinations, *without* bootstrapping character matrices (the most computationally heavy part of the INFER workflow). To accomplish this, we need to prep our `user_params.json` file with both
I/O and AFM pre-processing parameters.

```json
{  
    "afm_input" : "<path to --afm_input .csv file>",
    "output_folder" : "<path to TUNE output folder>",
    "path_meta" : "<path to cells metadata .csv file>", 
    "lineage_column" : "GBC",
    "min_n_positive" : [5],
    "af_confident_detection": [0.02, 0.03, 0.04],
    "min_n_confidently_detected": [2,5],
    "min_mean_AD_in_positives": [1,1.25,1.5],
    "t_prob": [0.7],
    "min_AD": [1,2],
    "min_cell_prevalence": [0.1],
    "bin_method": ["MiTo"]
}
```

Note that, for the TUNE entrypoint, the AFM preprocessing options `--af_confident_detection`, `--min_mean_AD_in_positives`, `--min_mean_AD_in_positives`, `--t_prob`, `--min_AD`, `--min_cell_prevalence`, `--bin_method` **must** be passed as arrays (even single-valued arrays). This is necessary to combine these options for Grid Search (i.e., all alternative combinations of parameters will be tested). Moreover, for this dataset we specified for the `lineage_column` option. This will allow nf-MiTo to properly recognize the `GBC` column in cell metadata as ground truth clonal labels. In turn, this parameter choice will include, among the other lineage inference evaluation metrics:

1. The Adjusted Rand Index (ARI) between MiTo clones and GBC (lentiviral clones) labels
2. The Normalized Mutual Information (NMI) between MiTo clones and GBC (lentiviral clones) labels
3. The % of MT-SNVs that is significantly enriched in a ground truth clone
4. The Area Under Precisio Recall Curve (AUPRC) computed as in [Ludwig et al. 2019](10.1016/j.cell.2019.01.022)

Once `user.config` are set (see [PREPROCESS tutorial](1.PREPROCESS.md)), we can launch `nf-MiTo` TUNE:

```bash
nextflow run main.nf -profile docker,local -params-file user_params.json -entry TUNE
```

After a successfull run, the output folder `<path to output folder>` should contains all the following outputs:

```
TUNE
├── all_metrics_final.csv
└── all_options_final.csv
```

These tables report (for all parameters combination tested, n=36 in this case) the chosen parameters and the associated metrics (`all_options_final.csv` and `all_metrics_final.csv`, respectively). Each parameter combination (and its metrics) is associated to a unique `job_id`. 

We will now use utilities from [MiTo](https://github.com/andrecossa5/MiTo) to look at these metrics and choose some candidate MT-SNV space for this sample.

First, install MiTo following the installation section of its documentation. 

Then, activate the MiTo environment and:

```python
import mito as mt

# Specify for the TUNE result folder
path_tuning = "<your TUNE --output_folder path>"

# Load merged df, metrics and options
df, metrics, options = mt.ut.format_tuning(path_tuning) 

# Look at the merged tuning df table structure
df.head()
df.columns

# Rank alternative jobs by ARI
ranked_df = df[['sample', 'job_id', 'ARI']].sort_values('ARI', ascending=False)

# Save top 3 MT-SNVs spaces, for the EXPLORE entrypoint
ranked_df.head(3)['job_id'].to_csv('chosen_jobs.csv')
```

The 3 best parameter combinations of this TUNE run were `job_id` `e2fb7d77af`, `d61d6fa350` and `ba5a7df293`.
We can now take advantage of the EXPLORE entrypoint to produce comprehensive visualizations.

# EXPLORE

For our 3 candidate MT-SNVs spaces (identified by their unique `job_id` value), nf-MiTo EXPLORE:

1. Loads the associated (unfiltered) AFM and MT-genome coverage table
2. Retrieves `job_id`'s parameters (i.e., from TUNE output folder, see `--path_tuning` below)
3. Re-performs AFM pre-processing and lineage inference
4. Produces comprehensive visualizations

In order to do this, two new input CSV files are needed: `--afm_input` and `--coverage_input`.

For the `--afm_input` file, nf-MiTo EXPLORE needs the usual CSV file format:

<table>
<thead>
<tr>
<th align="left">job_id</th>
<th align="left">sample</th>
<th align="left">afm</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><code>e2fb7d77af</code></td>
<td align="left"><code>MDA_clones<code></td>
<td align="left"><code>path to AFM</code></td>
</tr>
<td align="left"><code>d61d6fa350</code></td>
<td align="left"><code>MDA_clones<code></td>
<td align="left"><code>path to AFM</code></td>
</tr>
<td align="left"><code>ba5a7df293</code></td>
<td align="left"><code>MDA_clones<code></td>
<td align="left"><code>path to AFM</code></td>
</tr>
</tbody>
</table>

However, this time each row links one of our chosen `job_id`s to the corresponding sample name and unfiltered AFM. 
Similarly the `--coverage_input` file needs to be structured as:

<table>
<thead>
<tr>
<th align="left">job_id</th>
<th align="left">sample</th>
<th align="left">coverage</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><code>e2fb7d77af</code></td>
<td align="left"><code>MDA_clones<code></td>
<td align="left"><code>path to coverage</code></td>
</tr>
<td align="left"><code>d61d6fa350</code></td>
<td align="left"><code>MDA_clones<code></td>
<td align="left"><code>path to coverage</code></td>
</tr>
<td align="left"><code>ba5a7df293</code></td>
<td align="left"><code>MDA_clones<code></td>
<td align="left"><code>path to coverage</code></td>
</tr>
</tbody>
</table>

with rows linking `job_id`s to the corresponding samples name and coverage tables (see nf-MiTo [nf-MiTo PREPROCESS](1.PREPROCESS.md) output, with `--pp_method` == `maegatk`).

These 2 files will be now included in a new `user.params` file, along with with the additional `--path_tuning` option.

```json
{  
    "afm_input" : "<path to new --afm_input .csv file, with chosen jobs>",
    "coverage_input" : "<path to --coverage_input .csv file, with samples' coverage tables>",
    "output_folder" : "<path to EXPLORE output folder>",
    "path_meta" : "<path to cells metadata .csv file>", 
    "path_tuning" : "<path to the TUNE output folder>",
    "lineage_column" : "GBC",
}
```

For each `job_id`, nf-MiTo will be able to retrieve:

* the input AFM
* the MT-coverage table
* the tested parameter combination of choice

We run nf-MiTo EXPLORE with the command:

```bash
nextflow run main.nf -profile docker,local -params-file user_params.json -entry EXPLORE
```

After a successfull run, the EXPLORE output_folder should present the following structure:

```
└── MDA_clones
    ├── ba5a7df293
    │   ├── distances.png
    │   ├── embeddings.png
    │   ├── MT_SNVs.png
    │   ├── mut_profile.png
    │   ├── phylo_muts.png
    │   └── phylo.png
    ├── d61d6fa350
    │   ├── distances.png
    │   ├── embeddings.png
    │   ├── MT_SNVs.png
    │   ├── mut_profile.png
    │   ├── phylo_muts.png
    │   └── phylo.png
    └── e2fb7d77af
        ├── distances.png
        ├── embeddings.png
        ├── MT_SNVs.png
        ├── mut_profile.png
        ├── phylo_muts.png
        └── phylo.png
```

with each `job_id` sub-folder and the corresponding plots.  

# INFER

From visual inspection, 

We can and re-perform lineage inference one last time, bootstrapping each MT-SNV space character matrix, performing lineage inference, and gathering the complete set of tree metrics implemented in nf-MiTo.

With `user.settings` specifying:

```json
{  
    "afm_input" : "<path to new --afm_input .csv file, with chosen jobs>",
    "output_folder" : "<path to INFER output folder>",
    "path_meta" : "<path to cells metadata .csv file>", 
    "path_tuning" : "<path to the TUNE output folder>",
    "lineage_column" : "GBC",
}
```

we can now launch nf-MiTo INFER:

```bash
nextflow run main.nf -profile docker,local -params-file user_params.json -entry INFER
```

This final output folder will be structured as any other INFER run:

```
└── MDA_clones
    ├── ba5a7df293
    │   ├── afm_filtered.h5ad
    │   ├── annotated_tree.pickle
    │   └── tree_metrics.csv
    ├── d61d6fa350
    │   ├── afm_filtered.h5ad
    │   ├── annotated_tree.pickle
    │   └── tree_metrics.csv
    └── e2fb7d77af
        ├── afm_filtered.h5ad
        ├── annotated_tree.pickle
        └── tree_metrics.csv
```

See also the [INFER tutorial](2.INFER.md) for details. See [MiTo](https://github.com/andrecossa5/MiTo) documentation for interactive visualization and analysis tutorials.

# Additional remarks

This combination of workflows is designed to maximize the efficiency with which one can test alternative AFM pre-processing strategies and make lineage inferences. 

Some additional remarks:

* the EXPLORE workflow need MT-coverage information across the whole MT-genome. Only pre-processing pipelines that output that type of information (i.e., `maegatk`/`mito_preprocessing`) are suitable for this line of work. 

* choosing between alternative MT-SNVs spaces and phylogenies can be challenging, withot ground truth labels. In this scenario, one should look at the metrics that evaluate the single-cell character and phylogeny quality (i.e., number of characters per cell, correlation between cell-cell genetic and phylogenetic distance, Retention Index and Consistency index of characters along the phylogeny, etc). Together with visual interpretation of phylogeny dendrograms and MiTo clones, one can spot promising solutions (i.e., nice clade separations, branches highly supported by characters, few unassigned cells etc) from artifact-driven ones. In any case, as in other single-cell analysis fields, careful exploration and interpretation of data will always play a fundamental role.


